{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61886f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored successfully!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Step 1: Inspect the Data\n",
    "with open(\"beyond-good-and-evil.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Optional: Print first 1000 characters to inspect\n",
    "# print(text[:1000])\n",
    "\n",
    "# Step 2: Data Cleaning (this is minimal, adjust based on inspection)\n",
    "text = text.strip()  # Remove leading and trailing whitespace\n",
    "\n",
    "# Step 3: Segmentation\n",
    "paragraphs = [p.strip() for p in text.split(\"\\n\") if p]\n",
    "\n",
    "# Step 4: Store the Data in SQLite\n",
    "\n",
    "# Setting up SQLite\n",
    "conn = sqlite3.connect('philosophy_chatbot_data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Creating a Table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS philosophy_text (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    paragraph TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Inserting Data\n",
    "for para in paragraphs:\n",
    "    cursor.execute(\"INSERT INTO philosophy_text (paragraph) VALUES (?)\", (para,))\n",
    "\n",
    "# Commit and Close\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data stored successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d536df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('philosophy_chatbot_data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT paragraph FROM philosophy_text\")\n",
    "data = cursor.fetchall()\n",
    "\n",
    "# Convert data into a list of paragraphs\n",
    "paragraphs = [item[0] for item in data]\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea8be978",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7272b90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631f00691aeb471bb4375264d5b5ae12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ef9d2c959e41048812b2e5780cc1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02611bfb0b0f4e3ab40cc8a432b1006f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad4d072f5b04c45b0c18a2d16f5d003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35376854a9ae4ff9b2a23e51a7fa08cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2-medium\"  # You can choose other versions like \"gpt2-large\" if you have the computational resources\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2a3979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does Nietzsche say about our relationship with God? He argues that our relationship with God is a matter of the good and bad, which we can understand as a matter of the good and the bad, the good and the bad. He argues that we have a right to be angry at God for what he did to us. He argues that we have a right to be angry at God for what he did to us, and we should be angry at him for what he does to us. He argues that we have a right to be angry at God for what he did to us, and we should be angry at him for what he does to us, but this is only possible if we are willing to deal with God in a way that is reconciling to his goodness, or in a way that is reconciling to his evil. He argues that we have a right to be angry at God for what he did to us, and we should be angry at him for what he does to us,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "input_text = \"What does Nietzsche say about\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "attention_mask = torch.ones_like(input_ids)  # Creating an attention mask directly as a tensor\n",
    "\n",
    "# Ensure the tensors are of type long\n",
    "input_ids = input_ids.long()\n",
    "attention_mask = attention_mask.long()\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(input_ids, max_length=200, num_return_sequences=1, temperature=0.7, do_sample=True, attention_mask=attention_mask)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb63439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinethinks",
   "language": "python",
   "name": "machinethinks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
